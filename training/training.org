#+TITLE: Data Engineering Training Materials

* Introduction

All of the code and documentation for the Covid Act Now (CAN) scrapers,
including this document, is in the [[https://github.com/covid-projections/can-scrapers][can-scrapers]] Github repository. We will refer
to various folders within this repository throughout this document.

* COVID Data

** Cases and deaths

Case data can be split into two categories

1. /Confirmed cases|deaths/: This typically refers to individuals who have
   tested positive for COVID (typically via a [[PCR test]])
2. /Suspected (Probable) cases|deaths/: This refers to an individual, one who
   was typically hospitalized, who had many of the symptoms associated with
   COVID but that had not been administered a test to confirm the diagnosis.
   This variable definition was especially relevant early in the pandemic when
   there was not wide-spread testing.

In our work, we will mostly focus on /total cases|deaths/ which includes the sum
of confirmed and suspected cases|deaths -- This is because now that we have
wide-spread testing, there are fewer suspected cases because they can easily be
confirmed

** Hospitalizations
** Tests

*** Test types

There are three main kinds of tests antibody, antigen, and molecular. Antigen
and molecular are both types of diagnostic tests and we'll group them together
for our explanation below:

**** Antibody tests

Antibody tests, also known as serology tests, are described by the [[https://www.fda.gov/consumers/consumer-updates/coronavirus-disease-2019-testing-basics][FDA]] as,

#+BEGIN_QUOTE
An antibody test looks for antibodies that are made by your immune system in
response to a threat, such as a specific virus. Antibodies can help fight
infections. Antibodies can take several days or weeks to develop after you have
an infection and may stay in your blood for several weeks or more after
recovery. Because of this, antibody tests should not be used to diagnose
COVID-19. At this time researchers do not know if the presence of antibodies
means that you are immune to COVID-19 in the future. #+END_QUOTE

**** Diagnostic tests

Diagnostic tests are themselves broken into two sub-categories:

1. Molecular tests (in COVID context, often called a PCR test) <<PCR test>>
2. Antigen tests

Diagnostic tests are described by the [[https://www.fda.gov/consumers/consumer-updates/coronavirus-disease-2019-testing-basics][FDA]] as,

#+BEGIN_QUOTE
A diagnostic test can show if you have an active coronavirus infection and
should take steps to quarantine or isolate yourself from others. Currently there
are two types of diagnostic tests– molecular tests, such as RT-PCR tests, that
detect the virus’s genetic material, and antigen tests that detect specific
proteins from the virus. #+END_QUOTE

*** Measurements

There has been a significant amount of confusion about the units that testing
data is being reported in, see [[https://covidtracking.com/blog/test-positivity-in-the-us-is-a-mess][the COVID Tracking Project's blog post]] on the
topic. There are three main ways that COVID testing data is measured,

1. Specimens tested: The number of specimens that the labs collected/tested in a
   given geography
2. Test encounters: The number of unique people tested in a particular day for a
   given geography
3. Unique people: The number of unique people ever tested for a given geography

Lets work through an example so we can be specific about the difference:

Imagine that Alice and Bob both go to get tested on April 1, 2020. Two specimen
samples are collected from Alice and two specimen samples are collected from
Bob. Alice and Bob then go to a Memorial Day celebration at the end of May and
find out that the host tested COVID positive so they both go to get tested again
on June 1, 2020. An additional two specimens are collected from both Alice and
Bob.

- How many specimens do Alice and Bob contribute to the total? Both Alice and
  Bob would contribute two specimens from April and two specimens from June for
  a total of *eight specimens*.
- How many test encounters do Alice and Bob contribute to the total? Both Alice
  and Bob would contribute a single test encounter from April and a single test
  encounter from June for a total of *four test encounters*.
- How many unique people do Alice and Bob contribute to the total? Although
  Alice and Bob were each tested twice, they are the same person in April and
  June, so they would only contribute *two unique people* to the total.

The preferred metric at CAN is (currently) to count PCR test encounters. This
will not always be available so its important that we collect whatever is
available even if it isn't what we "want".

* Scraper Infrastructure

The overall microservice architecture we use is shown in the diagram below:

#+CAPTION: Infrastructure flow chart
#+NAME: fig:IFC
https://raw.githubusercontent.com/covid-projections/can-scrapers/33268d564f9d8b62d927ffa63d3d844a92b0efeb/docs/infrastructure/can_scrapers_overview.png

In, words, these components are:

1. Scrapers: these are open source scrapers written in Python. The repository is
   here https://github.com/covid-projections/can-scrapers
2. Database: we store all data in a postgrestql database
3. API: We have REST and GraphQL APIs. They are automatically generated using
   the PostgREST and postgraphile libraries
4. Client Libraries: We have client libraries in Python, R, and Julia that
   integrate with the REST API
5. API Gateway: we have a Kong API gateway that sits in front of all user
   requests and handles things like caching, routing, and authentication
6. Other services: we have a handful of other microservices that perform
   specific functions. These are contained in docker containers that communicate
   over HTTP and are managed by Google Cloud run

* Database

All of the data that is collected is stored in a PostgreSQL database hosted on
Google Cloud. This database is structured in three schemas:

1. =api= : This is the public facing schema. It does not contain any tables
   itself but rather contains views and materialized views
2. =data= : This schema is where data that is collected is stored
3. =meta= : This schema contains meta information about geographies and
   variables. It is information that will only be changed/updated infrequently.

** =data.covid_official=

The web scrapers we write place data in the table =data.covid_official=

We'll review this table and then fill in the details on the foreign key
relationships

#+BEGIN_SRC sql

  CREATE TABLE data.covid_official (
    vintage TIMESTAMP, dt DATE,
    location_id BIGINT REFERENCES meta.locations (id),
    variable_id SMALLINT REFERENCES meta.covid_variables (id),
    demographic_id SMALLINT REFERENCES
    meta.covid_demographics (id),
    value REAL,
    provider INT REFERENCES data.covid_providers (id) NOT NULL,
    PRIMARY KEY (vintage, dt, location_id, variable_id, demographic_id)
  );

#+END_SRC

[[file:../db/schemas/003_covid_individual_sources.sql::1][COVID File]]

The foreign key relationships are

- The =meta.locations= table keeps track of all "locations" in our database. For
  US states and counties this includes the US FIPS code
- The =meta.covid_variables= table contains information on the variable, its
  units, and form of measurement. We'll talk through this one in more detail
- The =meta.covid_demographics= contains information on the age, race, ethnicity, 
  and sex for individuals represented in the data observations.
- =data.covid_providers= helps us keep track where this data comes from. We'll
  look at this one also...

** =meta.covid_variables=

#+begin_src sql
CREATE TABLE meta.covid_variables
(
    id SERIAL PRIMARY KEY,
    category TEXT REFERENCES meta.covid_categories (subcategory),
    measurement TEXT REFERENCES meta.covid_measurement (name),
    unit TEXT REFERENCES meta.covid_unit (name)
);
#+end_src

Let's look at the [[file:~/valorum/covid/can-scrapers/db/schemas/002_covid_data.sql::CREATE TABLE meta.covid_variables][table]]

** =data.covid_providers=

Finally the =data.covid_providers= table

#+begin_src sql
  CREATE TABLE data.covid_providers
  (
      id SERIAL PRIMARY KEY,
      name TEXT UNIQUE,
      priority INT
  );
  priority INT );

#+end_src

* Getting Started, Development Notes

** Creating a development environment

1. Install =conda= (either anaconda or miniconda)
2. Create a conda environment for this project, =conda create -n can-scrapers
   python=3.6=
3. Activate the environment, =conda activate can-scrapers=
4. Move your command line or terminal into the =can-scrapers= directory
5. Install the required packages, =pip install -r requirements-dev.txt=
6. Install development version of the =can_tools= package, =pip install -e .=

** Running tests locally
*** Types of tests

    We have written automated unit and integration tests using the =DatasetBase=
    parent class.

    The unit tests do the following:

    - Verify that the =fetch= method runs without any network errors
    - Verify that the =normalize= method returns a DataFrame that
      - Is not empty
      - Has the correct columns
    - The =validate= method runs without any issues and returns =True=

    The integration tests test that the =put= method can successfully load the
    data into a test instance of the database (see below).

*** Running tests

    To run the full suite of unit tests, execute the following from the root of
    the =can-scrapers= repository:

    #+begin_src
pytest -v .
    #+end_src

    Suppose you were working on a scraper for Wyoming and =Wyoming= was in the
    name of the scraper class

    To run tests only for =Wyoming= you could do

    #+begin_src
pytest -v -k Wyoming .
    #+end_src

**** Extra: Using PostgreSQL

     Our production environment uses PostgreSQL as the database engine.

     All SQL interactions happen via sqlalchemy, which is (mostly) database
     backend agnostic.

     We can run integration tests either via an in-memory SQLite that is created
     on each run of the tests, or against a running postgres instance.

     Using SQLite while developing is encouraged as it is easier to get up and
     running.

     If you do choose to use PostgreSQL, please follow the instructions below
     for using Docker to set up the instance of postgres.

     Then you need to set the environment variable =CAN_PG_CONN_STR=. To do this
     in a unix environment (Linux or OSX), run the following command

    #+begin_src
export CAN_PG_CONN_STR="postgresql://postgres:postgres_test@localhost:5432"
    #+end_src

    If you are using Windows, set the environment variable by running

    #+begin_src
set CAN_PG_CONN_STR="postgresql://postgres:postgres_test@localhost:5432"
    #+end_src

    With the environment variable set, you can repeat the =pytest= commands from
    above

***** Local database instance using Docker

     We use a docker based workflow for running a local instance of the
     database.

     In order to utilize this workflow please start a postgresql container and
     leave it running

     Here's a sample =docker run= command that should do the trick

     #+begin_src shell
     docker run --rm -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres_test postgres:12
     #+end_src

** Setting up VS Code

   Steps to set up VS code:

   - Install =python= and =pylance= VS code extensions
   - Reload vs code window
   - Open =can-scrapers= directory in VS code
   - Select the =can-tools= conda environment as the workspace interpreter.

   Please do not push any changes made to the =.vscode= directory. That has some
   shared settings, but will also be overwritten by the absolute path to the
   conda environment on your machine. This path is unlikely to match exactly
   with the path for any other team members

* Scraper Library

The scrapers are defined by 4 operations:

1. Fetch: Retrieves raw data from dashboard
2. Normalize: Ingests raw data and spits out normalized data
3. Validate: Makes sure that the new normalized data is sensible
4. Put: Puts data into our database

#+CAPTION: Scraper flow chart
#+NAME: fig:CANSRAPERS
[[file:static/CAN_scrapers.png]]

** =DatasetBase= and relevant subclasses

*** =DatasetBase=

This is the most important base class

It is found in =can-scrapers/can_tools/scrapers/base.py=

*Methods that must be defined*:

- =fetch=
- =normalize=

*Methods that you are likely to use**

- =_retrieve_vintage=
- =_retrieve_dt=
- =extract_CMU=

**Methods that will be defined for you**

- =put=
- =validate=: a very basic version
- =

*** =StateDashboard= or =CountyDashboard=

This is another class that you are likely to use and is used when we don't have
another subclass that specializes in extracting data from that particular
dashboard

It is found in =can-scrapers/can_tools/scrapers/official/base.py=

*** =ArcGIS=

This subclass specializes in extracting information from an ArcGIS dashboard
(which are most of the current scrapers).

*Properties that must be defined*

- =ARCGIS_ID=

*Methods that you are likely to use*

- =get_all_jsons=
- =arcgis_jsons_to_df=

* Writing a new scraper

As seen in [[Scraper Library]], a scraper requires 4 methods:

1. =fetch=
2. =normalize=
3. =validate=
4. =put=

Most scrapers will *not* require one to write the =validate= or =put= methods
because the generic methods should be able to validate the data and dump it into
the database

* Example Scrapers

- [[file:../can_tools/scrapers/official/CA/ca_state.py][California State Dashboard Scraper (API Query)]]
- [[file:../can_tools/scrapers/official/FL/fl_state.py][Florida State Dashboard
  Scraper (ArcGIS)]]
